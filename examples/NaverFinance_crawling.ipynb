{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2f416b-47a9-4c1a-bcfd-30e7aa60394a",
   "metadata": {},
   "source": [
    "# NaverCrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7674e5e-df0b-4541-9e96-e51e4ccbf8af",
   "metadata": {},
   "source": [
    "네이버 뉴스에서 분야별 헤드라인 기사 수집\n",
    "- 정치: https://news.naver.com/section/100\n",
    "- 경제: https://news.naver.com/section/101\n",
    "- 사회: https://news.naver.com/section/102\n",
    "- 생활/문화: https://news.naver.com/section/103\n",
    "- IT/과학: https://news.naver.com/section/105\n",
    "- 세계: https://news.naver.com/section/104\n",
    "- 뉴스검색 OpenAPI: https://openapi.naver.com/v1/search/news.json?query=xxxx&display=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db0862-f158-44b1-86ce-db4f8520daed",
   "metadata": {},
   "source": [
    "```\n",
    "NaverCrawler\n",
    "├── NaverNews (뉴스 클래스)\n",
    "│   ├── fetch (질문으로 Naver News API를 이용하여 뉴스 목록 조회)\n",
    "│   └── parse (뉴스 목록으로 뉴스 상세 정보 조회)\n",
    "├── NaverMarket (주식 클래스)\n",
    "│   ├── fetch (기업 코드로 시작일과 종료일 사이 일별 시세를 조회)\n",
    "│   ├── fetch_market_sum (현재 종목의 시가총액을 스크래핑하여 숫자로 반환)\n",
    "│   ├── fetch_company_metadata (현재 종목의 시가총액을 스크래핑하여 숫자로 반환)\n",
    "│   └── parse (...)\n",
    "├── ...\n",
    "├── check_gcp (GCP에서 Caching 정보 확인)\n",
    "└── save_gcp (GCP에서 Caching 정보 저장)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1663c5f-d1d9-4c91-b813-4cd2456ef142",
   "metadata": {},
   "source": [
    "## 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3817ab2-2541-4131-8102-40bd8d1bad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from bs4 import BeautifulSoup # type: ignore\n",
    "import httpx # type: ignore\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd # type: ignore\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "from urllib import parse\n",
    "from typing import Dict, Any, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a36de8-b84b-4eee-9b38-eb8d0c0b0a91",
   "metadata": {},
   "source": [
    "#### Jupyter Notebook 설정\n",
    "\n",
    "Jupyter Notebook에서 필요한 라이브러리 로딩을 위한 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255d13b-23ce-4d3c-809c-241e3172a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef1a2e-7b22-4f9e-8b0f-7fda7fdab6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.companydict import companydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba43a4-8abf-4703-b6ec-3c8ceef1d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gcpmanager import BQManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef62d78-44e6-40fd-baab-4963779e43f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb5963-1cf8-47aa-84e6-d63a23aa3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class News:\n",
    "    \"\"\"Independent Naver news pipeline (no NaverCrawler dependency).\"\"\"\n",
    "    def __init__(self, bq_manager: Optional[BQManager] = None):\n",
    "        if not bq_manager:\n",
    "            bq_manager = BQManager()\n",
    "        self.bq_manager = bq_manager\n",
    "            \n",
    "        self.client = httpx.AsyncClient(headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'\n",
    "        }, follow_redirects=True)\n",
    "\n",
    "    async def collect(self, query: str, max_articles: int = 100):\n",
    "        table_id = f\"news-naver-{query}\"\n",
    "\n",
    "        enc_text = parse.quote(query)\n",
    "        api_url = f\"https://openapi.naver.com/v1/search/news.json?query={enc_text}&display={max_articles}\"\n",
    "\n",
    "        client_id = 'YOUR_NAVER_CLIENT_ID'\n",
    "        client_secret = 'YOUR_NAVER_CLIENT_SECRET'\n",
    "        \n",
    "        api_headers = {\n",
    "            \"X-Naver-Client-Id\": client_id,\n",
    "            \"X-Naver-Client-Secret\": client_secret\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = await self.client.get(api_url, headers=api_headers)\n",
    "            response.raise_for_status()\n",
    "            search_result = response.json()\n",
    "            news_list = search_result.get('items', [])\n",
    "            yield {\"type\": \"progress\", \"step\": \"api_call\", \"status\": \"done\", \"total\": len(news_list)}\n",
    "\n",
    "        except Exception as e:\n",
    "            yield {\"type\": \"error\", \"message\": f\"API request failed: {e}\"}\n",
    "            return\n",
    "\n",
    "        scraped_treasures: list[dict] = []\n",
    "        total_articles = len(news_list)\n",
    "        for i, news_item in enumerate(news_list):\n",
    "            news_url = news_item.get('link')\n",
    "            if not news_url or 'news.naver.com' not in news_url:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                yield {\"type\": \"progress\", \"step\": \"scraping\", \"current\": i + 1, \"total\": total_articles}\n",
    "                response = await self.client.get(news_url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, HTML_PARSER)\n",
    "\n",
    "                title = soup.select_one('h2#title_area')\n",
    "                content = soup.select_one('div#newsct_article')\n",
    "                press = soup.select_one('img.media_end_head_top_logo_img') \n",
    "\n",
    "                cleaned_title = title.get_text(strip=True) if title else \"제목 없음\"\n",
    "                cleaned_content = content.get_text(strip=True) if content else \"본문 없음\"\n",
    "                cleaned_press = press['alt'] if press and 'alt' in press.attrs else \"언론사 불명\"\n",
    "\n",
    "                treasure_box = {\n",
    "                    'search_keyword': query,\n",
    "                    'original_link': news_url,\n",
    "                    'title': cleaned_title,\n",
    "                    'press': cleaned_press,\n",
    "                    'content': cleaned_content[:500],\n",
    "                    'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }\n",
    "                scraped_treasures.append(treasure_box)\n",
    "                await asyncio.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {news_url}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not scraped_treasures:\n",
    "            yield {\"type\": \"result\", \"data\": []}\n",
    "            return\n",
    "\n",
    "        yield {\"type\": \"progress\", \"step\": \"saving\", \"status\": \"saving to BigQuery\"}\n",
    "        _ = self._prepare_and_save_news_data(scraped_treasures, table_id)\n",
    "        yield {\"type\": \"result\", \"data\": {\"saved\": len(scraped_treasures)}}\n",
    "\n",
    "    async def process(self, query: str, limit: int | None = None):\n",
    "        table_id = f\"news-naver-{query}\"\n",
    "        cached_df = self.bq_manager.query_table(table_id=table_id, order_by_date=False)\n",
    "        if cached_df is None or cached_df.empty:\n",
    "            yield {\"type\": \"result\", \"data\": []}\n",
    "            return\n",
    "        if 'crawled_at' in cached_df.columns:\n",
    "            cached_df['crawled_at'] = pd.to_datetime(cached_df['crawled_at']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        cached_df.fillna('', inplace=True)\n",
    "        if 'content' in cached_df.columns:\n",
    "            cached_df['content'] = cached_df['content'].str.slice(0, 500)\n",
    "        if limit is not None:\n",
    "            cached_df = cached_df.head(limit)\n",
    "        yield {\"type\": \"result\", \"data\": cached_df.to_dict(orient='records')}\n",
    "\n",
    "    def _prepare_and_save_news_data(self, treasures: list[dict], table_id: str) -> pd.DataFrame:\n",
    "        df_treasures = pd.DataFrame(treasures)\n",
    "        self.bq_manager.load_dataframe(\n",
    "            df=df_treasures,\n",
    "            table_id=table_id,\n",
    "            if_exists=\"append\",\n",
    "            deduplicate_on=['original_link']\n",
    "        )\n",
    "        return df_treasures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c160d-63d1-4ed8-a7ba-54ce939434a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Old Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb5695-5436-471f-b2a5-1b1e64924ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Market:\n",
    "    def __init__(self, \n",
    "                 bq_manager: Optional[BQManager] = None, \n",
    "                 company_dict: Optional[Any] = None, \n",
    "                 company: Optional[str] = None):\n",
    "        \n",
    "        if company_dict:\n",
    "            self.company_dict = company_dict\n",
    "        \n",
    "        if bq_manager:\n",
    "            self.bq_manager = bq_manager\n",
    "        else:\n",
    "            self.bq_manager = BQManager()\n",
    "\n",
    "        self._header = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',\n",
    "            'Accept' : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\"\n",
    "        }\n",
    "        self.base_url = 'https://finance.naver.com'\n",
    "        self.company = company or '005930'\n",
    "        self.bq_manager = bq_manager\n",
    "        self.company_dict = company_dict\n",
    "        self.client = httpx.AsyncClient(headers=self._header, follow_redirects=True)\n",
    "\n",
    "    async def market_collect(self, company: str | None = None, start_date: str | None = None, end_date: str | None = None, max_page: int = 10):\n",
    "        if company:\n",
    "            self.company = company\n",
    "\n",
    "        if not end_date:\n",
    "            end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        if not start_date:\n",
    "            start_date = (datetime.now() - timedelta(days=180)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        company_name = self.company_dict.get_company_by_code(self.company) or self.company\n",
    "        table_id = f\"market-naverfinance-{company_name}\"\n",
    "\n",
    "        crawled_df = pd.DataFrame()\n",
    "        async for progress_update in _crawl_price_history(self.company, self.client, max_page=max_page):\n",
    "            if progress_update[\"type\"] == \"progress\":\n",
    "                yield progress_update\n",
    "            elif progress_update[\"type\"] == \"result\":\n",
    "                crawled_df = progress_update[\"data\"]\n",
    "\n",
    "        if crawled_df.empty:\n",
    "            yield {\"type\": \"error\", \"message\": \"Failed to crawl market data.\"}\n",
    "            return\n",
    "\n",
    "        # Filter by date range\n",
    "        crawled_df = crawled_df[\n",
    "            (crawled_df['date'] >= pd.to_datetime(start_date)) &\n",
    "            (crawled_df['date'] <= pd.to_datetime(end_date))\n",
    "        ]\n",
    "\n",
    "        yield {\"type\": \"progress\", \"step\": \"saving\", \"status\": \"saving to BigQuery\"}\n",
    "        df_saved = self._prepare_and_save_market_data(crawled_df, table_id)\n",
    "        yield {\"type\": \"result\", \"data\": {\"saved\": len(df_saved)}}\n",
    "    \n",
    "    def _prepare_and_save_market_data(self, df: pd.DataFrame, table_id: str) -> pd.DataFrame:\n",
    "        \"\"\"Clean market dataframe and persist to BigQuery in one step.\n",
    "\n",
    "        - Ensures required columns and types\n",
    "        - Adds code/source columns\n",
    "        - Saves to BigQuery with deduplication\n",
    "        - Returns the dataframe that was saved\n",
    "        \"\"\"\n",
    "        df_for_bq = df.copy()\n",
    "        if 'date' in df_for_bq.columns:\n",
    "            df_for_bq['date'] = pd.to_datetime(df_for_bq['date']).dt.date\n",
    "\n",
    "        for col in ['open', 'high', 'low', 'close', 'volume']:\n",
    "            if col in df_for_bq.columns:\n",
    "                df_for_bq[col] = pd.to_numeric(df_for_bq[col], errors='coerce').fillna(0)\n",
    "                if col == 'volume':\n",
    "                    df_for_bq[col] = df_for_bq[col].astype('int64')\n",
    "                else:\n",
    "                    df_for_bq[col] = df_for_bq[col].astype(float)\n",
    "\n",
    "        df_for_bq['code'] = self.company\n",
    "        df_for_bq['source'] = 'naver'\n",
    "\n",
    "        self.bq_manager.load_dataframe(\n",
    "            df=df_for_bq,\n",
    "            table_id=table_id,\n",
    "            if_exists=\"append\",\n",
    "            deduplicate_on=['date', 'code']\n",
    "        )\n",
    "\n",
    "        return df_for_bq\n",
    "\n",
    "    async def _get_market_cap(self):\n",
    "        \"\"\"현재 종목의 시가총액을 스크래핑하여 숫자로 반환합니다.\"\"\"\n",
    "        try:\n",
    "            url = f'https://finance.naver.com/item/sise.naver?code={self.company}'\n",
    "            response = await self.client.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, HTML_PARSER)\n",
    "            \n",
    "            market_sum_tag = soup.select_one('#_market_sum')\n",
    "            if market_sum_tag:\n",
    "                market_sum_text = market_sum_tag.get_text(strip=True)\n",
    "                \n",
    "                market_sum = 0\n",
    "                parts = market_sum_text.replace(',', '').split('조')\n",
    "                if len(parts) > 1:\n",
    "                    market_sum += int(parts[0]) * 1_0000_0000_0000\n",
    "                    remaining = parts[1]\n",
    "                else:\n",
    "                    remaining = parts[0]\n",
    "                \n",
    "                if '억' in remaining:\n",
    "                    market_sum += int(remaining.replace('억', '')) * 1_0000_0000\n",
    "                \n",
    "                return market_sum\n",
    "            return 0\n",
    "        except Exception:\n",
    "            return 0\n",
    "    \n",
    "    async def market_process(self, company: str | None = None):\n",
    "        if company:\n",
    "            self.company = company\n",
    "        \n",
    "        company_name = self.company_dict.get_company_by_code(self.company) or self.company\n",
    "        table_id = f\"market-naverfinance-{company_name}\"\n",
    "        \n",
    "        cached_df = self.bq_manager.query_table(table_id=table_id, order_by_date=True)\n",
    "        \n",
    "        if cached_df is None or cached_df.empty:\n",
    "            yield {\"type\": \"result\", \"data\": {}}\n",
    "            return\n",
    "\n",
    "        formatted_data = await self._format_response_from_df(cached_df)\n",
    "        yield {\"type\": \"result\", \"data\": formatted_data}\n",
    "\n",
    "    async def _format_response_from_df(self, df: pd.DataFrame):\n",
    "        company_name = self.company_dict.get_company_by_code(self.company) or self.company\n",
    "        market_cap = await self._get_market_cap()\n",
    "\n",
    "        if df is None or df.empty:\n",
    "            return {\n",
    "                \"name\": company_name,\n",
    "                \"source\": \"naver\",\n",
    "                \"currentPrice\": {\"value\": 0, \"changePercent\": 0},\n",
    "                \"volume\": {\"value\": 0, \"changePercent\": 0},\n",
    "                \"marketCap\": {\"value\": market_cap, \"changePercent\": 0},\n",
    "                \"priceHistory\": [],\n",
    "                \"volumeHistory\": [],\n",
    "            }\n",
    "            \n",
    "        df.sort_values(by='date', ascending=False, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        latest = df.iloc[0]\n",
    "        previous = df.iloc[1] if len(df) > 1 else latest\n",
    "\n",
    "        price_change_percent = ((latest['close'] - previous['close']) / previous['close']) * 100 if previous['close'] != 0 else 0\n",
    "        volume_change_percent = ((latest['volume'] - previous['volume']) / previous['volume']) * 100 if previous['volume'] != 0 else 0\n",
    "\n",
    "        latest_close = float(latest['close']) if pd.notna(latest['close']) else 0.0\n",
    "        latest_volume = int(latest['volume']) if pd.notna(latest['volume']) else 0\n",
    "\n",
    "        result = {\n",
    "            \"name\": company_name or self.company,\n",
    "            \"source\": \"naver\",\n",
    "            \"currentPrice\": {\n",
    "                \"value\": latest_close,\n",
    "                \"changePercent\": round(price_change_percent, 2)\n",
    "            },\n",
    "            \"volume\": {\n",
    "                \"value\": latest_volume,\n",
    "                \"changePercent\": round(volume_change_percent, 2)\n",
    "            },\n",
    "            \"marketCap\": {\n",
    "                \"value\": market_cap,\n",
    "                \"changePercent\": 0 \n",
    "            },\n",
    "            \"priceHistory\": df.rename(columns={'close': 'price'})[['date', 'price']].to_dict(orient='records'),\n",
    "            \"volumeHistory\": df[['date', 'volume']].to_dict(orient='records')\n",
    "        }\n",
    "\n",
    "        for item in result['priceHistory']:\n",
    "            if isinstance(item['date'], pd.Timestamp):\n",
    "                item['date'] = item['date'].strftime('%Y-%m-%d')\n",
    "\n",
    "        for item in result['volumeHistory']:\n",
    "            if isinstance(item['date'], pd.Timestamp):\n",
    "                item['date'] = item['date'].strftime('%Y-%m-%d')\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857d7288-d607-4415-8fed-d528911b93d3",
   "metadata": {},
   "source": [
    "## NaverNewsDetail\n",
    "\n",
    "네이버 뉴스 상세 클래스\n",
    "\n",
    "제목, 내용, 언론사, 입력일, 기자 목록, 카테고리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47a016-cedf-4b7f-b6eb-f0790fa19383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNewsDetail:\n",
    "    \n",
    "    # 제목\n",
    "    title_selectors = [\n",
    "        '#title_area span',\n",
    "        '#ct .media_end_head_headline',\n",
    "        '.media_end_head_headline',\n",
    "        'h2#title_area',\n",
    "        '.news_end_title'\n",
    "    ]\n",
    "\n",
    "    # 본문\n",
    "    content_selectors = [\n",
    "        '#dic_area',\n",
    "        'article#dic_area',\n",
    "        '.go_trans._article_content',\n",
    "        '._article_body_contents'\n",
    "    ]\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        url: str | None = None\n",
    "    ):\n",
    "        self.client = httpx.AsyncClient(headers=self.headers, follow_redirects=True)\n",
    "        self.url = url\n",
    "\n",
    "    async def fetch(\n",
    "        self,\n",
    "        article: Dict | None = None\n",
    "    ):\n",
    "        news_url = article.get('url')\n",
    "        \n",
    "        try:\n",
    "            print(f\"News Detail URL: {news_url}\")\n",
    "            response = await self.client.get(news_url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # 뉴스 상세 정보 파싱\n",
    "            _article = self._parse_news(response.text)\n",
    "            \n",
    "            article.update(_article)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {news_url}: {e}\")\n",
    "\n",
    "        finally:\n",
    "            return article\n",
    "\n",
    "    def _parse_news(\n",
    "        self,\n",
    "        html_text: str\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        HTML에서 뉴스 상세 정보를 파싱한다.\n",
    "        제목, 본문 내용, 언론사, 입력일, 기자 목록\n",
    "\n",
    "        Args:\n",
    "            html_text (str): HTML 텍스트\n",
    "\n",
    "        Returns:\n",
    "            뉴스 상세 Dictionary\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "        # 제목\n",
    "        title = self._parse_title(soup)\n",
    "        # 본문 내용\n",
    "        content = self._parse_content(soup)\n",
    "        # 언론사\n",
    "        press = self._parse_press(soup)\n",
    "        # 입력일\n",
    "        published_date = self._parse_published_date(soup)\n",
    "        # 기자 목록\n",
    "        authors = self._parse_authors(soup)\n",
    "        # 카테고리\n",
    "        category = self._parse_category(soup)\n",
    "\n",
    "        article = {\n",
    "            'title': title,\n",
    "            'content': content,\n",
    "            'press': press,\n",
    "            'authors': (\", \").join(authors),\n",
    "            'category': category,\n",
    "            'published_date': published_date,\n",
    "            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "        #print(f\"Article: {article}\")\n",
    "        return article\n",
    "\n",
    "    def _parse_title(\n",
    "        self,\n",
    "        soup\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HTML에서 제목을 파싱한다.\n",
    "\n",
    "        Args:\n",
    "            soup:\n",
    "\n",
    "        Returns:\n",
    "            제목 문자열\n",
    "        \"\"\"\n",
    "        title = \"제목 없음\"\n",
    "        \n",
    "        for selector in self.title_selectors:\n",
    "            try:\n",
    "                title_element = soup.select_one(selector)\n",
    "                title = title_element.get_text(strip=True)\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return title\n",
    "\n",
    "    def _parse_content(\n",
    "        self,\n",
    "        soup\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HTML에서 본문 내용을 파싱한다.\n",
    "\n",
    "        Args:\n",
    "            soup:\n",
    "\n",
    "        Returns:\n",
    "            본문 내용 문자열\n",
    "        \"\"\"\n",
    "        content = \"본문 없음\"\n",
    "        \n",
    "        for selector in self.content_selectors:\n",
    "            try:\n",
    "                content_element = soup.select_one(selector)\n",
    "                content = content_element.get_text(strip=True)\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return content\n",
    "\n",
    "    def _parse_press(\n",
    "        self,\n",
    "        soup\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HTML에서 언론사를 파싱한다.\n",
    "\n",
    "        Args:\n",
    "            soup:\n",
    "\n",
    "        Returns:\n",
    "            언론사 문자열\n",
    "        \"\"\"\n",
    "        press = \"언론사 불명\"\n",
    "        \n",
    "        try:\n",
    "            press_element = soup.select_one('a.media_end_head_top_logo img')\n",
    "            press = press_element.get('alt')\n",
    "        except:\n",
    "            try:\n",
    "                press_element = soup.select_one('.media_end_head_top_logo_text')\n",
    "                press = press_element['alt']\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return press\n",
    "\n",
    "    def _parse_published_date(\n",
    "        self,\n",
    "        soup\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HTML에서 뉴스 입력일을 파싱한다.\n",
    "\n",
    "        Args:\n",
    "            soup:\n",
    "\n",
    "        Returns:\n",
    "            뉴스 입력일 문자열\n",
    "        \"\"\"\n",
    "        published_date = \"뉴스 입력일 불명\"\n",
    "        \n",
    "        try:\n",
    "            date_element = soup.select_one('span.media_end_head_info_datestamp_time')\n",
    "            published_date = date_element.get('data-date-time')\n",
    "        except:\n",
    "            published_date = datetime.now().strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "        return published_date\n",
    "\n",
    "    def _parse_authors(\n",
    "        self,\n",
    "        soup\n",
    "    ) -> List:\n",
    "        \"\"\"\n",
    "        HTML에서 기자 목록을 파싱한다.\n",
    "\n",
    "        Args:\n",
    "            soup:\n",
    "\n",
    "        Returns:\n",
    "            기자 목록\n",
    "        \"\"\"\n",
    "        authors = []\n",
    "        \n",
    "        try:\n",
    "            #author_elements = soup.select('em.media_end_head_journalist_name')\n",
    "            #author = author_element.get_text(strip=True)\n",
    "            \n",
    "            author_elements = soup.select('span.byline_s')\n",
    "            for author_element in author_elements:\n",
    "                author = author_element.get_text(strip=True)\n",
    "                authors.append(author)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "        return authors\n",
    "    \n",
    "    def _parse_category(\n",
    "        self,\n",
    "        soup\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        HTML에서 카테고리를 파싱한다.\n",
    "\n",
    "        Args:\n",
    "            soup:\n",
    "\n",
    "        Returns:\n",
    "            카테고리\n",
    "        \"\"\"\n",
    "        authors = []\n",
    "        \n",
    "        try:\n",
    "            category_element = soup.select_one('em.media_end_categorize_item')\n",
    "            category = category_element.get_text(strip=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "        return category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de9685-ec1a-4e17-a81c-af1b00a75e9e",
   "metadata": {},
   "source": [
    "## NaverNews\n",
    "\n",
    "뉴스 검색 클래스\n",
    "\n",
    "- 검색을 통한 뉴스 조회\n",
    "- 카테고리 뉴스 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c7870-161f-4c8a-a620-580ae31cb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNews:\n",
    "    \"\"\"\n",
    "    뉴스 검색 클래스\n",
    "    \n",
    "    Independent Naver news pipeline (no NaverCrawler dependency).\n",
    "    \"\"\"\n",
    "\n",
    "    news_categories = {\n",
    "        '정치': 'https://news.naver.com/section/100',\n",
    "        '경제': 'https://news.naver.com/section/101',\n",
    "        '사회': 'https://news.naver.com/section/102',\n",
    "        '생활/문화': 'https://news.naver.com/section/103',\n",
    "        'IT/과학': 'https://news.naver.com/section/105',\n",
    "        '세계': 'https://news.naver.com/section/104'\n",
    "    }\n",
    "\n",
    "    max_per_category = 10\n",
    "    news_openapi_url = \"https://openapi.naver.com/v1/search/news.json?query={query}&display={display}\"\n",
    "\n",
    "    client_id = 'EOof636e7yvLvMe3t1jg'\n",
    "    client_secret = 'lb4v_qXkRI'\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = httpx.AsyncClient(headers=self.headers, follow_redirects=True)\n",
    "\n",
    "    async def fetch(\n",
    "        self,\n",
    "        category: str = \"조회\",\n",
    "        query: str | None = None,\n",
    "        url: str | None = None,\n",
    "        max_articles: int = 100\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        질문으로 Naver News API를 뉴스 목록을 조회한다.\n",
    "        카테고리에 대한 뉴스 목록을 조회한다.\n",
    "        \n",
    "        Args:\n",
    "            category (str): 조회, 카테고리 (정치, 경제, 사회, 생활/문화, IT/과학, 세계). 기본값: 조회\n",
    "            query (str): 기사 검색 문구\n",
    "            url (str): 뉴스 검색 URL\n",
    "            max_articles (int): 최대 뉴스 갯수\n",
    "\n",
    "        Returns:\n",
    "            뉴스 Dictionary 목록\n",
    "        \"\"\"\n",
    "\n",
    "        if category != \"조회\":\n",
    "            return self._fetch_category_news()\n",
    "        \n",
    "        url = self.news_openapi_url if not url else url\n",
    "        enc_text = parse.quote(query)\n",
    "        api_url = url.format(query=enc_text, display=max_articles)\n",
    "        \n",
    "        print(f\"News 목록 URL: {api_url}\")\n",
    "\n",
    "        api_headers = {\n",
    "            \"X-Naver-Client-Id\": self.client_id,\n",
    "            \"X-Naver-Client-Secret\": self.client_secret\n",
    "        }\n",
    "\n",
    "        articles = []\n",
    "        try:\n",
    "            response = await self.client.get(api_url, headers=api_headers)\n",
    "            response.raise_for_status()\n",
    "            search_result = response.json()\n",
    "\n",
    "            # JSON에서 link 정보 추출\n",
    "            news_list = search_result.get('items', [])\n",
    "            #print(f\"Items: {news_list}\")\n",
    "            for news_item in news_list:\n",
    "                article = {\n",
    "                    'query': query,\n",
    "                    'url': news_item.get(\"link\")\n",
    "                }\n",
    "                articles.append(article)\n",
    "            \n",
    "            return articles\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "        \n",
    "        return articles\n",
    "\n",
    "    async def parse(\n",
    "        self,\n",
    "        articles: List[Dict]\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        뉴스 목록으로 뉴스 상세 정보를 가져온다.\n",
    "        제목, 내용, 언론사, 입력일, 기자 목록\n",
    "        카테고리 X\n",
    "\n",
    "        Args:\n",
    "            news_list (List): 신문 기사 URL 목록\n",
    "\n",
    "        Returns:\n",
    "            뉴스 상세 Dictionary 목록\n",
    "        \"\"\"\n",
    "\n",
    "        for idx, article in enumerate(articles):\n",
    "            news_url = article.get('url')\n",
    "            \n",
    "            if news_url and 'news.naver.com' in news_url:\n",
    "                detail = NaverNewsDetail()\n",
    "                article = await detail.fetch(article)\n",
    "            \n",
    "            await asyncio.sleep(random.uniform(0.1, 0.3))\n",
    "\n",
    "        return articles\n",
    "\n",
    "    def _fetch_category_news(self):\n",
    "        \"\"\"\n",
    "        카테고리별 뉴스 목록을 가져온다.\n",
    "        \"\"\"\n",
    "        \n",
    "        articles = []\n",
    "        for category_name, category_url in categories.items():\n",
    "            print(f\"News 목록 URL: {category_url}\")\n",
    "            article_links = get_article_links(driver, category_url, self.max_per_category)\n",
    "\n",
    "            for url in article_links:\n",
    "                article = {\n",
    "                    'query': category_name,\n",
    "                    'url': url\n",
    "                }\n",
    "                articles.append(article)\n",
    "\n",
    "        return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacac88-6d81-4311-944c-cb55d9f16868",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://openapi.naver.com/v1/search/news.json?query={query}&display={display}\"\n",
    "\n",
    "news = NaverNews()\n",
    "#news_list = await news.fetch(\"삼성전자\", url)\n",
    "#news_list = await news.fetch(query = \"삼성전자\")\n",
    "news_list = await news.fetch(query = \"삼성전자\", max_articles=10)\n",
    "news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043490a9-1493-4ee9-b0b5-2eee02c6b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = await news.parse(news_list)\n",
    "news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462c32c-f206-464f-b71d-9123ddb2c643",
   "metadata": {},
   "source": [
    "#### NaverMarket\n",
    "\n",
    "네이버 주식 클래스\n",
    "\n",
    "기업 코드로 시작일과 종료일 사이 일별 시세를 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d4e5bb-fcb1-4569-bccb-61adf10deacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverMarket:\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',\n",
    "        'Accept' : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\"\n",
    "    }\n",
    "\n",
    "    base_url = 'https://finance.naver.com'\n",
    "\n",
    "    def __init__(self, \n",
    "        code: Optional[str] = '005930'):\n",
    "        \n",
    "        self.code = code\n",
    "        self.client = httpx.AsyncClient(headers=self.headers, follow_redirects=True)\n",
    "\n",
    "    async def fetch(\n",
    "        self,\n",
    "        code: str | None = None, \n",
    "        start_date: str | None = None, \n",
    "        end_date: str | None = None, \n",
    "        max_page: int = 100\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        기업 코드로 시작일과 종료일 사이 일별 시세를 조회한다.\n",
    "        한 페이지의 크기는 10으로 고정되어 있음 (Gemini 답변)\n",
    "\n",
    "        Args:\n",
    "            code (str): 기업 코드, 예: 005930 (삼성전자)\n",
    "            start_date (str): 시작일, 예: 2025-09-01\n",
    "            end_date (str): 종료일, 예: 2025-12-31\n",
    "            max_page (int): 최대 페이지 수\n",
    "\n",
    "        Returns:\n",
    "            일별 시세 DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        if code:\n",
    "            self.code = code\n",
    "\n",
    "        if not end_date:\n",
    "            end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        if not start_date:\n",
    "            start_date = (datetime.now() - timedelta(days=180)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        market_prices = await self._fetch_historical_data(code, max_page=max_page)\n",
    "        print(market_prices, type(market_prices))\n",
    "\n",
    "        if market_prices.empty:\n",
    "            print(\"type:\", \"error\", \",\", \"message:\", \"Failed to crawl market data.\")\n",
    "            return\n",
    "\n",
    "        # Filter by date range\n",
    "        market_prices = market_prices[\n",
    "            (market_prices['date'] >= pd.to_datetime(start_date)) &\n",
    "            (market_prices['date'] <= pd.to_datetime(end_date))\n",
    "        ]\n",
    "\n",
    "        return market_prices\n",
    "        \n",
    "    async def _fetch_historical_data(\n",
    "        self,\n",
    "        code: str,\n",
    "        start_date: str | None = None,\n",
    "        max_page: int = 99999\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        기업 코드로 시작일부터 현재까지 일별 시세를 조회한다.\n",
    "        한글 키를 영어로 변환. 예: '날짜': 'date'\n",
    "\n",
    "        Args:\n",
    "            code (str): 기업 코드, 예: 005930 (삼성전자) \n",
    "            start_date (str): 시작일, 예: 2025-09-01\n",
    "            max_page (int): 최대 페이지 수\n",
    "\n",
    "        Returns:\n",
    "            일별 시세 DataFrame\n",
    "        \"\"\"\n",
    "        last_page = max_page\n",
    "        \n",
    "        full_df = []\n",
    "        page = 1\n",
    "        while True:\n",
    "            try:\n",
    "                url = f\"{base_url}/item/sise_day.nhn?code={code}&page={page}\"\n",
    "                print(f\"Parsing URL: {url}\")\n",
    "                response = await self.client.get(url)\n",
    "                response.raise_for_status()\n",
    "        \n",
    "                if last_page == max_page:\n",
    "                    _last_page = self._find_last_page(response.text)\n",
    "                    last_page = min(max_page, _last_page)\n",
    "                    #print(_last_page, last_page)\n",
    "        \n",
    "                dfs = pd.read_html(StringIO(response.text))\n",
    "                df = dfs[0]\n",
    "                df.dropna(how='all', inplace=True)\n",
    "                \n",
    "                if df.empty:\n",
    "                    break\n",
    "                \n",
    "                full_df.append(df)\n",
    "        \n",
    "                if start_date:\n",
    "                    # 현재 페이지에서 가장 과거 날짜 확인\n",
    "                    # 네이버 금융 날짜 포맷은 'YYYY.MM.DD' 이므로 문자열 비교 가능\n",
    "                    min_date = df['날짜'].min()\n",
    "                    #print(f\"Min Date: {min_date}\")\n",
    "                    \n",
    "                    # 현재 페이지의 가장 옛날 날짜가 설정한 시작일보다 작거나 같으면\n",
    "                    # 더 과거로 갈 필요가 없으므로 루프 종료\n",
    "                    if min_date <= start_date:\n",
    "                        print(f\"Reached start_date limit: {min_date} <= {start_date}\")\n",
    "                        break\n",
    "                \n",
    "                print(page, last_page)\n",
    "                if page >= last_page:\n",
    "                    break\n",
    "        \n",
    "                page += 1\n",
    "                await asyncio.sleep(random.uniform(0.1, 0.3))\n",
    "            except Exception as exc:\n",
    "                print(f\"Error scraping page {page}: {exc}\")\n",
    "                continue\n",
    "                \n",
    "        crawled_df = pd.concat(full_df, ignore_index=True)\n",
    "        crawled_df.rename(columns={\n",
    "            '날짜': 'date',\n",
    "            '종가': 'close',\n",
    "            '시가': 'open',\n",
    "            '고가': 'high',\n",
    "            '저가': 'low',\n",
    "            '거래량': 'volume'\n",
    "        }, inplace=True)\n",
    "        \n",
    "        if '전일비' in crawled_df.columns:\n",
    "            crawled_df.drop(columns=['전일비'], inplace=True)\n",
    "        \n",
    "        numeric_cols = ['close', 'open', 'high', 'low', 'volume']\n",
    "        for col in numeric_cols:\n",
    "            if col in crawled_df.columns:\n",
    "                crawled_df[col] = pd.to_numeric(\n",
    "                    crawled_df[col].astype(str).str.replace(',', '', regex=False),\n",
    "                    errors='coerce'\n",
    "                ).fillna(0)\n",
    "                if col == 'volume':\n",
    "                    crawled_df[col] = crawled_df[col].astype('int64')\n",
    "        \n",
    "        crawled_df['date'] = pd.to_datetime(crawled_df['date'], errors='coerce')\n",
    "        crawled_df.dropna(subset=['date'], inplace=True)\n",
    "        return crawled_df\n",
    "\n",
    "    def _find_last_page(\n",
    "        self,\n",
    "        html_text: str\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        기업 일별 시세에 대한 최대 페이지를 확인\n",
    "\n",
    "        Args:\n",
    "            html_text (str): HTML 텍스트에서 마지막 페이지 번호 확인\n",
    "\n",
    "        Returns:\n",
    "            마지막 페이지 번호\n",
    "        \"\"\"\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        match = None\n",
    "        pg_rr_tag = soup.select_one('.pgRR a')\n",
    "        \n",
    "        if pg_rr_tag:\n",
    "            href_value = pg_rr_tag.get('href')\n",
    "            if isinstance(href_value, str):\n",
    "                match = re.search(r'page=(\\d+)', href_value)\n",
    "    \n",
    "        if match:\n",
    "            last_page = int(match.group(1))\n",
    "        else:\n",
    "            last_page = 1\n",
    "    \n",
    "        return last_page\n",
    "\n",
    "    async def fetch_market_sum(\n",
    "        self,\n",
    "        code: str | None = None, \n",
    "    ):\n",
    "        \"\"\"\n",
    "        현재 종목의 시가총액을 스크래핑하여 숫자로 반환합니다.\n",
    "\n",
    "        Args:\n",
    "            code (str): 기업 코드, 예: 005930 (삼성전자)\n",
    "\n",
    "        Returns:\n",
    "            시가 총액 정수값\n",
    "        \"\"\"\n",
    "\n",
    "        if code:\n",
    "            self.code = code\n",
    "        \n",
    "        try:\n",
    "            url = f'{self.base_url}/item/sise.naver?code={self.code}'\n",
    "            response = await self.client.get(url)\n",
    "            response.raise_for_status()\n",
    "    \n",
    "            soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "            # HTML 텍스트\n",
    "            market_sum_tag = soup.select_one('#_market_sum')\n",
    "    \n",
    "            # Tag 존재하지 않음\n",
    "            if not market_sum_tag:\n",
    "                return 0\n",
    "            \n",
    "            market_sum_text = market_sum_tag.get_text(strip=True)\n",
    "            \n",
    "            market_sum = 0\n",
    "            parts = market_sum_text.replace(',', '').split('조')\n",
    "            if len(parts) > 1:\n",
    "                market_sum += int(parts[0]) * 1_0000_0000_0000\n",
    "                remaining = parts[1]\n",
    "            else:\n",
    "                remaining = parts[0]\n",
    "            \n",
    "            if '억' in remaining:\n",
    "                market_sum += int(remaining.replace('억', '')) * 1_0000_0000\n",
    "    \n",
    "            return market_sum\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    async def fetch_company_metadata(\n",
    "        self,\n",
    "        code: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        기업의 메타데이터를 조회합니다.\n",
    "        기업명, 주식 거래소, 환율, 시가 총액 등\n",
    "\n",
    "        Args:\n",
    "            code (str): 기업 코드, 예: 005930 (삼성전자)\n",
    "\n",
    "        Returns:\n",
    "            기업 메타데이터 Dictionary\n",
    "        \"\"\"\n",
    "        metadata: Dict[str, Any] = {}\n",
    "        latest_price: float | None = None\n",
    "\n",
    "        try:\n",
    "            url = f\"https://m.stock.naver.com/api/stock/{code}/basic\"\n",
    "            print(f\"Parsing URL: {url}\")\n",
    "            response = await self.client.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            basic_json = response.json()\n",
    "\n",
    "            # 주식 이름\n",
    "            metadata[\"company_name\"] = basic_json.get(\"stockName\")\n",
    "\n",
    "            # 주식 거래 형태\n",
    "            exchange_info = basic_json.get(\"stockExchangeType\") or {}\n",
    "            metadata[\"exchange\"] = (\n",
    "                exchange_info.get(\"name\")\n",
    "                or basic_json.get(\"stockExchangeName\")\n",
    "            )\n",
    "            # 환율\n",
    "            metadata[\"currency\"] = (\n",
    "                self._infer_currency(exchange_info.get(\"nationCode\"))\n",
    "                or \"KRW\"\n",
    "            )\n",
    "\n",
    "            closing_price = basic_json.get(\"closePrice\")\n",
    "            if closing_price is not None:\n",
    "                try:\n",
    "                    latest_price = float(str(closing_price).replace(',', ''))\n",
    "                except ValueError:\n",
    "                    latest_price = None\n",
    "        except Exception as exc:\n",
    "            metadata.setdefault(\"_errors\", {})[\"basic\"] = str(exc)\n",
    "    \n",
    "        try:\n",
    "            url = f\"https://api.finance.naver.com/service/itemSummary.naver?itemcode={code}\"\n",
    "    \n",
    "            headers = self.headers\n",
    "            headers['Referer'] = f'https://finance.naver.com/item/main.nhn?code={code}'\n",
    "            print(f\"Parsing URL: {url}\")\n",
    "            \n",
    "            response = await self.client.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            summary_json = response.json()\n",
    "            market_sum = summary_json.get(\"marketSum\")\n",
    "            if isinstance(market_sum, (int, float)):\n",
    "                market_cap = float(market_sum) * 1_000_000  # marketSum is in million KRW\n",
    "                metadata[\"market_cap\"] = market_cap\n",
    "                if latest_price and latest_price > 0:\n",
    "                    metadata[\"shares_outstanding\"] = int(round(market_cap / latest_price))\n",
    "            else:\n",
    "                metadata.setdefault(\"_warnings\", []).append(\"marketSum missing\")\n",
    "\n",
    "        except Exception as exc:\n",
    "            metadata.setdefault(\"_errors\", {})[\"summary\"] = str(exc)\n",
    "    \n",
    "        return metadata\n",
    "\n",
    "    def _infer_currency(self, nation_code: str | None) -> str | None:\n",
    "        if not nation_code:\n",
    "            return None\n",
    "        nation_code = nation_code.upper()\n",
    "        if nation_code in {'KOR', 'KR'}:\n",
    "            return 'KRW'\n",
    "        if nation_code in {'USA', 'US'}:\n",
    "            return 'USD'\n",
    "        if nation_code in {'JPN', 'JP'}:\n",
    "            return 'JPY'\n",
    "        if nation_code in {'CHN', 'CN'}:\n",
    "            return 'CNY'\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "258edd92-ec90-4f9d-9670-805d1bec8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '005930' # 삼성전자\n",
    "market = NaverMarket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9083f-7d7d-412d-9858-ae60723b23d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "market_df = await market.fetch(code, start_date='2025-09-01', end_date='2025-12-31')\n",
    "market_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84861bdb-4ad9-40bf-92e8-9d762363e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_sum = await market.fetch_market_sum(code)\n",
    "market_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b03bd1-7a5d-442d-a457-177190986254",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = await market.fetch_company_metadata(code)\n",
    "metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
