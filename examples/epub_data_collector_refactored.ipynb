{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3f35a7-9e46-4626-8824-83de4053bca9",
   "metadata": {},
   "source": [
    "# EPUB 영어-한국어 문장 정렬 도구\n",
    "\n",
    "영어와 한국어 EPUB 파일을 챕터 및 문장 단위로 정렬하여\n",
    "번역 학습 데이터셋(JSONL)을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0678473-0378-4eb0-b789-b239c3ef46fc",
   "metadata": {},
   "source": [
    "```\n",
    "EpubAligner\n",
    "├── TextProcessor (텍스트 전처리 및 문장 분리)\n",
    "│   ├── split_sentences (언어별 문장 분리)\n",
    "│   └── clean_html (HTML에서 텍스트 추출)\n",
    "├── EpubLoader (EPUB 파일 로더)\n",
    "│   ├── load (EPUB에서 챕터 추출)\n",
    "│   └── _extract_chapter (단일 챕터 추출)\n",
    "├── DPAligner (동적 프로그래밍 기반 정렬)\n",
    "│   ├── load (EPUB에서 챕터 추출)\n",
    "│   ├── align_chapters (챕터 레벨 정렬)\n",
    "│   ├── align_sentences (문장 레벨 정렬 (N:M 매칭 지원))\n",
    "│   ├── _compute_chapter_embeddings (챕터 임베딩 계산 (처음 N문장 요약))\n",
    "│   ├── _dp_align (기본 DP 정렬 (1:1 매칭))\n",
    "│   ├── _build_sentence_dp (문장 정렬용 DP 테이블 (N:M 매칭))\n",
    "│   └── _backtrack_sentences (문장 정렬 역추적)\n",
    "├── OutputWriter (결과 파일 작성기)\n",
    "│   ├── write_header\n",
    "│   ├── write_chapter_header\n",
    "│   └── write_result\n",
    "├── Chapter (챕터 데이터)\n",
    "├── AlignConfig (정렬 설정)\n",
    "├── AlignmentResult (정렬 결과)\n",
    "├── process_pair (단일 책 쌍 처리)\n",
    "└── run (전체 파이프라인 실행)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350e71d6-bf40-4731-9b59-60cce5146905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import re\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from colorama import init, Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55aabda0-bb1a-49f8-8464-5ff34c06bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from kiwipiepy import Kiwi\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee4eb75e-92d7-4a29-a950-17d6fd706e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화\n",
    "init(autoreset=True)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLTK 데이터 확인 (SSL 우회)\n",
    "def ensure_nltk_data():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt_tab')\n",
    "    except LookupError:\n",
    "        import ssl\n",
    "        try:\n",
    "            _create_unverified_https_context = ssl._create_unverified_context\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        else:\n",
    "            ssl._create_default_https_context = _create_unverified_https_context\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "ensure_nltk_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72b266-a562-45d2-9dcb-f1cdc2d0327f",
   "metadata": {},
   "source": [
    "## 설정 및 상수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b420ef1e-0a8f-43d3-974c-39f9557dce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AlignConfig:\n",
    "    \"\"\"정렬 설정\"\"\"\n",
    "    input_dir: str = 'data'\n",
    "    device: str = field(default_factory=lambda: 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    threshold: float = 0.65\n",
    "    max_merge_n: int = 5\n",
    "    merge_bonus: float = 0.001\n",
    "    \n",
    "    # 내부 상수\n",
    "    chapter_skip_penalty: float = -0.2\n",
    "    sentence_skip_penalty: float = -0.3\n",
    "    chapter_sim_threshold: float = 0.3\n",
    "    min_chapter_length: int = 50\n",
    "    chapter_summary_sentences: int = 10\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chapter:\n",
    "    \"\"\"챕터 데이터\"\"\"\n",
    "    idx: int\n",
    "    text: str\n",
    "    sentences: list[str]\n",
    "    embedding: torch.Tensor | None = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AlignmentResult:\n",
    "    \"\"\"정렬 결과\"\"\"\n",
    "    match_type: str  # '1:1', '1:2', '2:1', 'SKIP_EN', 'SKIP_KR'\n",
    "    en_indices: list[int]\n",
    "    kr_indices: list[int]\n",
    "    similarity: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08968db-fb5a-42ca-a29c-ab7bf07e0c72",
   "metadata": {},
   "source": [
    "## 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0616741a-4628-43da-9268-ef9a314f3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"텍스트 전처리 및 문장 분리\"\"\"\n",
    "    \n",
    "    REMOVE_TAGS = ['script', 'style', 'nav', 'sup', 'footer', 'header', 'table']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.kiwi = Kiwi()\n",
    "    \n",
    "    def clean_html(self, html: str) -> str:\n",
    "        \"\"\"HTML에서 텍스트 추출\"\"\"\n",
    "        if not html:\n",
    "            return \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        for tag in soup(self.REMOVE_TAGS):\n",
    "            tag.decompose()\n",
    "        \n",
    "        text = soup.get_text()\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    def split_sentences(self, text: str, lang: str) -> list[str]:\n",
    "        \"\"\"언어별 문장 분리\"\"\"\n",
    "        if lang == 'en':\n",
    "            sentences = sent_tokenize(text)\n",
    "        else:\n",
    "            sentences = [s.text for s in self.kiwi.split_into_sents(text)]\n",
    "        \n",
    "        return [s for s in sentences if len(s.strip()) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed760a-febd-49f5-82a7-17ba76ea13d1",
   "metadata": {},
   "source": [
    "## EPUB 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9307f51-0603-4a6c-a9f2-89e4d1c0aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpubLoader:\n",
    "    \"\"\"EPUB 파일 로더\"\"\"\n",
    "    \n",
    "    def __init__(self, processor: TextProcessor, config: AlignConfig):\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "    \n",
    "    def load(self, filepath: str, lang: str) -> list[Chapter]:\n",
    "        \"\"\"EPUB에서 챕터 추출\"\"\"\n",
    "        try:\n",
    "            book = epub.read_epub(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"{Fore.RED}[Error] Failed to read {filepath}: {e}{Style.RESET_ALL}\")\n",
    "            return []\n",
    "        \n",
    "        chapters = []\n",
    "        \n",
    "        for idx, (item_id, _) in enumerate(book.spine):\n",
    "            chapter = self._extract_chapter(book, item_id, idx, lang)\n",
    "            if chapter:\n",
    "                chapter.idx = len(chapters)\n",
    "                chapters.append(chapter)\n",
    "        \n",
    "        return chapters\n",
    "    \n",
    "    def _extract_chapter(self, book, item_id: str, idx: int, lang: str) -> Chapter | None:\n",
    "        \"\"\"단일 챕터 추출\"\"\"\n",
    "        item = book.get_item_with_id(item_id)\n",
    "        if not item or item.get_type() != ebooklib.ITEM_DOCUMENT:\n",
    "            return None\n",
    "        \n",
    "        content = item.get_content().decode('utf-8', errors='ignore')\n",
    "        cleaned = self.processor.clean_html(content)\n",
    "        \n",
    "        if len(cleaned) < self.config.min_chapter_length:\n",
    "            return None\n",
    "        \n",
    "        sentences = self.processor.split_sentences(cleaned, lang)\n",
    "        if not sentences:\n",
    "            return None\n",
    "        \n",
    "        return Chapter(idx=idx, text=cleaned, sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007667cb-dc58-4eec-956f-2972dd9823bb",
   "metadata": {},
   "source": [
    "## 동적 프로그래밍 정렬기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f648df36-9cd8-4044-a033-54ab8919d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPAligner:\n",
    "    \"\"\"동적 프로그래밍 기반 정렬\"\"\"\n",
    "    \n",
    "    def __init__(self, model: SentenceTransformer, config: AlignConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "    \n",
    "    def align_chapters(\n",
    "        self, \n",
    "        en_chapters: list[Chapter], \n",
    "        kr_chapters: list[Chapter]\n",
    "    ) -> list[tuple[Chapter, Chapter]]:\n",
    "        \"\"\"챕터 레벨 정렬\"\"\"\n",
    "        if not en_chapters or not kr_chapters:\n",
    "            return []\n",
    "        \n",
    "        n_en, n_kr = len(en_chapters), len(kr_chapters)\n",
    "        print(f\"{Fore.YELLOW}[Chapter] Aligning {n_en} EN ↔ {n_kr} KR chapters{Style.RESET_ALL}\")\n",
    "        \n",
    "        # 임베딩 계산\n",
    "        en_embs = self._compute_chapter_embeddings(en_chapters)\n",
    "        kr_embs = self._compute_chapter_embeddings(kr_chapters)\n",
    "        sim_matrix = util.cos_sim(en_embs, kr_embs).cpu().numpy()\n",
    "        \n",
    "        # DP 정렬\n",
    "        matches = self._dp_align(\n",
    "            sim_matrix, \n",
    "            skip_penalty=self.config.chapter_skip_penalty,\n",
    "            sim_threshold=self.config.chapter_sim_threshold\n",
    "        )\n",
    "        \n",
    "        return [(en_chapters[i], kr_chapters[j]) for i, j in matches]\n",
    "    \n",
    "    def align_sentences(\n",
    "        self, \n",
    "        en_sents: list[str], \n",
    "        kr_sents: list[str]\n",
    "    ) -> Iterator[AlignmentResult]:\n",
    "        \"\"\"문장 레벨 정렬 (N:M 매칭 지원)\"\"\"\n",
    "        if not en_sents or not kr_sents:\n",
    "            return\n",
    "        \n",
    "        # 임베딩 계산\n",
    "        en_emb = self.model.encode(en_sents, convert_to_tensor=True, device=self.config.device)\n",
    "        kr_emb = self.model.encode(kr_sents, convert_to_tensor=True, device=self.config.device)\n",
    "        sim_matrix = util.cos_sim(en_emb, kr_emb).cpu().numpy()\n",
    "        \n",
    "        # DP 테이블 구축\n",
    "        n_en, n_kr = len(en_sents), len(kr_sents)\n",
    "        dp, backtrack = self._build_sentence_dp(en_emb, kr_emb, sim_matrix, n_en, n_kr)\n",
    "        \n",
    "        # 역추적하여 결과 생성\n",
    "        yield from self._backtrack_sentences(dp, backtrack, sim_matrix, n_en, n_kr)\n",
    "    \n",
    "    def _compute_chapter_embeddings(self, chapters: list[Chapter]) -> torch.Tensor:\n",
    "        \"\"\"챕터 임베딩 계산 (처음 N문장 요약)\"\"\"\n",
    "        summaries = [\n",
    "            \" \".join(c.sentences[:self.config.chapter_summary_sentences]) \n",
    "            for c in chapters\n",
    "        ]\n",
    "        return self.model.encode(summaries, convert_to_tensor=True, device=self.config.device)\n",
    "    \n",
    "    def _dp_align(\n",
    "        self, \n",
    "        sim_matrix: np.ndarray, \n",
    "        skip_penalty: float,\n",
    "        sim_threshold: float = 0.0\n",
    "    ) -> list[tuple[int, int]]:\n",
    "        \"\"\"기본 DP 정렬 (1:1 매칭)\"\"\"\n",
    "        n_en, n_kr = sim_matrix.shape\n",
    "        \n",
    "        dp = np.full((n_en + 1, n_kr + 1), -np.inf)\n",
    "        backtrack = np.zeros((n_en + 1, n_kr + 1), dtype=int)\n",
    "        dp[0, 0] = 0\n",
    "        \n",
    "        # Actions: 0=Stop, 1=Match, 2=Skip_EN, 3=Skip_KR\n",
    "        for i in range(n_en + 1):\n",
    "            for j in range(n_kr + 1):\n",
    "                if i == 0 and j == 0:\n",
    "                    continue\n",
    "                \n",
    "                candidates = []\n",
    "                \n",
    "                if i > 0 and j > 0:\n",
    "                    candidates.append((dp[i-1, j-1] + sim_matrix[i-1, j-1], 1))\n",
    "                if i > 0:\n",
    "                    candidates.append((dp[i-1, j] + skip_penalty, 2))\n",
    "                if j > 0:\n",
    "                    candidates.append((dp[i, j-1] + skip_penalty, 3))\n",
    "                \n",
    "                if candidates:\n",
    "                    best_score, best_action = max(candidates, key=lambda x: x[0])\n",
    "                    dp[i, j] = best_score\n",
    "                    backtrack[i, j] = best_action\n",
    "        \n",
    "        # 역추적\n",
    "        matches = []\n",
    "        i, j = n_en, n_kr\n",
    "        \n",
    "        while i > 0 or j > 0:\n",
    "            action = backtrack[i, j]\n",
    "            if action == 0:\n",
    "                break\n",
    "            elif action == 1:\n",
    "                if sim_matrix[i-1, j-1] > sim_threshold:\n",
    "                    matches.append((i-1, j-1))\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            elif action == 2:\n",
    "                i -= 1\n",
    "            else:\n",
    "                j -= 1\n",
    "        \n",
    "        matches.reverse()\n",
    "        return matches\n",
    "    \n",
    "    def _build_sentence_dp(\n",
    "        self, \n",
    "        en_emb: torch.Tensor, \n",
    "        kr_emb: torch.Tensor,\n",
    "        sim_matrix: np.ndarray,\n",
    "        n_en: int, \n",
    "        n_kr: int\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"문장 정렬용 DP 테이블 (N:M 매칭)\"\"\"\n",
    "        max_n = self.config.max_merge_n\n",
    "        skip_penalty = self.config.sentence_skip_penalty\n",
    "        merge_bonus = self.config.merge_bonus\n",
    "        \n",
    "        dp = np.full((n_en + 1, n_kr + 1), -np.inf)\n",
    "        backtrack = np.zeros((n_en + 1, n_kr + 1), dtype=int)\n",
    "        dp[0, 0] = 0\n",
    "        \n",
    "        # Actions:\n",
    "        # 1: 1:1, 2: Skip_EN, 3: Skip_KR\n",
    "        # 10+k: 1:k (merge k KR), 20+k: k:1 (merge k EN)\n",
    "        \n",
    "        for r in range(n_en + 1):\n",
    "            for c in range(n_kr + 1):\n",
    "                if r == 0 and c == 0:\n",
    "                    continue\n",
    "                \n",
    "                candidates = []\n",
    "                \n",
    "                # Skip\n",
    "                if r > 0:\n",
    "                    candidates.append((dp[r-1, c] + skip_penalty, 2))\n",
    "                if c > 0:\n",
    "                    candidates.append((dp[r, c-1] + skip_penalty, 3))\n",
    "                \n",
    "                # 1:1 Match\n",
    "                if r > 0 and c > 0:\n",
    "                    candidates.append((dp[r-1, c-1] + sim_matrix[r-1, c-1], 1))\n",
    "                \n",
    "                # 1:k Match (merge KR)\n",
    "                for k in range(2, max_n + 1):\n",
    "                    if r > 0 and c >= k:\n",
    "                        vec_avg = torch.mean(kr_emb[c-k:c], dim=0)\n",
    "                        sim = util.cos_sim(en_emb[r-1], vec_avg).item()\n",
    "                        bonus = merge_bonus * (k - 1)\n",
    "                        candidates.append((dp[r-1, c-k] + sim + bonus, 10 + k))\n",
    "                \n",
    "                # k:1 Match (merge EN)\n",
    "                for k in range(2, max_n + 1):\n",
    "                    if r >= k and c > 0:\n",
    "                        vec_avg = torch.mean(en_emb[r-k:r], dim=0)\n",
    "                        sim = util.cos_sim(vec_avg, kr_emb[c-1]).item()\n",
    "                        bonus = merge_bonus * (k - 1)\n",
    "                        candidates.append((dp[r-k, c-1] + sim + bonus, 20 + k))\n",
    "                \n",
    "                if candidates:\n",
    "                    best_score, best_action = max(candidates, key=lambda x: x[0])\n",
    "                    dp[r, c] = best_score\n",
    "                    backtrack[r, c] = best_action\n",
    "        \n",
    "        return dp, backtrack\n",
    "    \n",
    "    def _backtrack_sentences(\n",
    "        self, \n",
    "        dp: np.ndarray, \n",
    "        backtrack: np.ndarray,\n",
    "        sim_matrix: np.ndarray,\n",
    "        n_en: int, \n",
    "        n_kr: int\n",
    "    ) -> Iterator[AlignmentResult]:\n",
    "        \"\"\"문장 정렬 역추적\"\"\"\n",
    "        results = []\n",
    "        r, c = n_en, n_kr\n",
    "        merge_bonus = self.config.merge_bonus\n",
    "        \n",
    "        while r > 0 or c > 0:\n",
    "            action = backtrack[r, c]\n",
    "            if action == 0:\n",
    "                break\n",
    "            \n",
    "            if action == 1:  # 1:1\n",
    "                sim = dp[r, c] - dp[r-1, c-1]\n",
    "                results.append(AlignmentResult('1:1', [r-1], [c-1], sim))\n",
    "                r -= 1\n",
    "                c -= 1\n",
    "            \n",
    "            elif action == 2:  # Skip EN\n",
    "                results.append(AlignmentResult('SKIP_EN', [r-1], [], 0))\n",
    "                r -= 1\n",
    "            \n",
    "            elif action == 3:  # Skip KR\n",
    "                c -= 1\n",
    "            \n",
    "            elif action >= 20:  # k:1 (merge EN)\n",
    "                k = action - 20\n",
    "                sim = dp[r, c] - dp[r-k, c-1] - merge_bonus * (k - 1)\n",
    "                results.append(AlignmentResult(f'{k}:1', list(range(r-k, r)), [c-1], sim))\n",
    "                r -= k\n",
    "                c -= 1\n",
    "            \n",
    "            elif action >= 10:  # 1:k (merge KR)\n",
    "                k = action - 10\n",
    "                sim = dp[r, c] - dp[r-1, c-k] - merge_bonus * (k - 1)\n",
    "                results.append(AlignmentResult(f'1:{k}', [r-1], list(range(c-k, c)), sim))\n",
    "                r -= 1\n",
    "                c -= k\n",
    "        \n",
    "        results.reverse()\n",
    "        yield from results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1948a6-12de-4bb3-bde7-a5b77383ec7a",
   "metadata": {},
   "source": [
    "## 출력 작성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978df35e-46cd-4ae9-8d86-d2234cf7b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputWriter:\n",
    "    \"\"\"결과 파일 작성기\"\"\"\n",
    "    \n",
    "    def __init__(self, json_path: str, log_path: str, config: AlignConfig):\n",
    "        self.json_file = open(json_path, 'w', encoding='utf-8')\n",
    "        self.log_file = open(log_path, 'w', encoding='utf-8')\n",
    "        self.config = config\n",
    "        self.match_count = 0\n",
    "    \n",
    "    def write_header(self, title: str):\n",
    "        self.log_file.write(f\"=== Log for {title} (Max Merge: {self.config.max_merge_n}) ===\\n\\n\")\n",
    "    \n",
    "    def write_chapter_header(self, en_idx: int, kr_idx: int):\n",
    "        self.log_file.write(f\"\\n### Chapter Pair: EN[{en_idx}] ↔ KR[{kr_idx}] ###\\n\")\n",
    "    \n",
    "    def write_result(\n",
    "        self, \n",
    "        result: AlignmentResult, \n",
    "        en_sents: list[str], \n",
    "        kr_sents: list[str]\n",
    "    ):\n",
    "        en_text = \" \".join(en_sents[i] for i in result.en_indices)\n",
    "        \n",
    "        if result.match_type == 'SKIP_EN':\n",
    "            self._write_skip(en_text, result.similarity)\n",
    "            return\n",
    "        \n",
    "        kr_text = \" \".join(kr_sents[i] for i in result.kr_indices)\n",
    "        is_match = result.similarity >= self.config.threshold\n",
    "        \n",
    "        self._write_log(result, en_text, kr_text, is_match)\n",
    "        \n",
    "        if is_match:\n",
    "            self._write_json(en_text, kr_text)\n",
    "            self.match_count += 1\n",
    "    \n",
    "    def _write_skip(self, en_text: str, sim: float):\n",
    "        self.log_file.write(\n",
    "            f\"[SKIP] (Score: {sim:.4f})\\n\"\n",
    "            f\" EN: {en_text}\\n\"\n",
    "            f\" KR: (No Match)\\n\"\n",
    "            f\"{'-'*50}\\n\"\n",
    "        )\n",
    "    \n",
    "    def _write_log(self, result: AlignmentResult, en_text: str, kr_text: str, is_match: bool):\n",
    "        status = \"MATCH\" if is_match else \"REJECT\"\n",
    "        self.log_file.write(\n",
    "            f\"[{status}] (Score: {result.similarity:.4f} | Type: {result.match_type})\\n\"\n",
    "            f\" EN: {en_text}\\n\"\n",
    "            f\" KR: {kr_text}\\n\"\n",
    "            f\"{'-'*50}\\n\"\n",
    "        )\n",
    "    \n",
    "    def _write_json(self, en_text: str, kr_text: str):\n",
    "        data = {\n",
    "            \"system\": \"전문 번역가 스타일로 번역하세요.\",\n",
    "            \"user\": en_text,\n",
    "            \"assistant\": kr_text\n",
    "        }\n",
    "        self.log_file.flush()\n",
    "        self.json_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    def close(self):\n",
    "        self.json_file.close()\n",
    "        self.log_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca82ddd-fdce-410c-884c-d50c5793ec1e",
   "metadata": {},
   "source": [
    "## 메인 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3feb2f22-c9f6-4e81-b774-2f88ad6405a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpubAligner:\n",
    "    \"\"\"EPUB 정렬 파이프라인\"\"\"\n",
    "    \n",
    "    def __init__(self, config: AlignConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        print(f\"{Fore.CYAN}[System] Loading models...{Style.RESET_ALL}\")\n",
    "        \n",
    "        self.processor = TextProcessor()\n",
    "        self.loader = EpubLoader(self.processor, config)\n",
    "        \n",
    "        model = SentenceTransformer('sentence-transformers/LaBSE', device=config.device)\n",
    "        model.eval()\n",
    "        self.aligner = DPAligner(model, config)\n",
    "    \n",
    "    def process_pair(self, en_path: str, kr_path: str):\n",
    "        \"\"\"단일 책 쌍 처리\"\"\"\n",
    "        title = Path(en_path).stem.replace('en_', '')\n",
    "        print(f\"\\n{Fore.BLUE}=== Processing: {title} ==={Style.RESET_ALL}\")\n",
    "        \n",
    "        # 챕터 로드\n",
    "        en_chapters = self.loader.load(en_path, 'en')\n",
    "        kr_chapters = self.loader.load(kr_path, 'kr')\n",
    "        print(f\" - Loaded: EN={len(en_chapters)}, KR={len(kr_chapters)} chapters\")\n",
    "        \n",
    "        # 챕터 정렬\n",
    "        matched_chapters = self.aligner.align_chapters(en_chapters, kr_chapters)\n",
    "        print(f\" - Matched: {len(matched_chapters)} chapter pairs\")\n",
    "        \n",
    "        # 문장 정렬 및 출력\n",
    "        writer = OutputWriter(\n",
    "            f\"dataset_{title}.jsonl\",\n",
    "            f\"dataset_{title}.log\",\n",
    "            self.config\n",
    "        )\n",
    "        writer.write_header(title)\n",
    "        \n",
    "        for en_chap, kr_chap in tqdm(matched_chapters, desc=\"Aligning\"):\n",
    "            writer.write_chapter_header(en_chap.idx, kr_chap.idx)\n",
    "            \n",
    "            for result in self.aligner.align_sentences(en_chap.sentences, kr_chap.sentences):\n",
    "                writer.write_result(result, en_chap.sentences, kr_chap.sentences)\n",
    "        \n",
    "        writer.close()\n",
    "        \n",
    "        print(f\"{Fore.GREEN} >> Done! Matches: {writer.match_count}{Style.RESET_ALL}\")\n",
    "        print(f\" >> Log: dataset_{title}.log\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"전체 파이프라인 실행\"\"\"\n",
    "        input_dir = Path(self.config.input_dir)\n",
    "        input_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        en_files = sorted(input_dir.glob(\"en_*.epub\"))\n",
    "        \n",
    "        for en_file in en_files:\n",
    "            kr_file = en_file.with_name(en_file.name.replace(\"en_\", \"kr_\"))\n",
    "            \n",
    "            if kr_file.exists():\n",
    "                self.process_pair(str(en_file), str(kr_file))\n",
    "            else:\n",
    "                print(f\"{Fore.YELLOW}[Skip] No pair for {en_file.name}{Style.RESET_ALL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9e5bb8-5ce5-4094-8dfc-27e12fd69df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--input_dir INPUT_DIR] [--device DEVICE]\n",
      "                             [--threshold THRESHOLD]\n",
      "                             [--max_merge_n MAX_MERGE_N]\n",
      "                             [--merge_bonus MERGE_BONUS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/seongjungkim/Library/Jupyter/runtime/kernel-b8804d04-143c-4f78-9a90-87485c472642.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "def parse_args() -> AlignConfig:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"EPUB 영어-한국어 문장 정렬 도구\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('--input_dir', default='data', help='EPUB 파일 디렉토리')\n",
    "    parser.add_argument('--device', default=None, help='PyTorch device (auto-detect if not set)')\n",
    "    parser.add_argument('--threshold', type=float, default=0.65, help='유사도 임계값')\n",
    "    parser.add_argument('--max_merge_n', type=int, default=5, help='최대 병합 문장 수')\n",
    "    parser.add_argument('--merge_bonus', type=float, default=0.001, help='병합 보너스')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    print(f\"args: {args}, type{args}\")\n",
    "    \n",
    "    config = AlignConfig(\n",
    "        input_dir=args.input_dir,\n",
    "        threshold=args.threshold,\n",
    "        max_merge_n=args.max_merge_n,\n",
    "        merge_bonus=args.merge_bonus\n",
    "    )\n",
    "    \n",
    "    if args.device:\n",
    "        config.device = args.device\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = parse_args()\n",
    "    aligner = EpubAligner(config)\n",
    "    aligner.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ef7773-bccc-4063-ba5a-b0cc81e25446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing: Harry Potter and the Goblet of Fire ===\n",
      " - Loaded: EN=38, KR=43 chapters\n",
      "[Chapter] Aligning 38 EN ↔ 43 KR chapters\n",
      " - Matched: 38 chapter pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 38/38 [19:49<00:00, 31.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Done! Matches: 5566\n",
      " >> Log: dataset_Harry Potter and the Goblet of Fire.log\n",
      "\n",
      "=== Processing: Harry Potter and the Sorcerors Stone ===\n",
      " - Loaded: EN=18, KR=22 chapters\n",
      "[Chapter] Aligning 18 EN ↔ 22 KR chapters\n",
      " - Matched: 17 chapter pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 17/17 [08:12<00:00, 28.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Done! Matches: 4485\n",
      " >> Log: dataset_Harry Potter and the Sorcerors Stone.log\n"
     ]
    }
   ],
   "source": [
    "config = AlignConfig(\n",
    "    input_dir=\"data\",\n",
    "    threshold=0.65,\n",
    "    max_merge_n=5,\n",
    "    merge_bonus=0.001\n",
    ")\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "aligner = EpubAligner(config)\n",
    "aligner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
