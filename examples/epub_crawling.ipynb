{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c251210e-e06c-47e3-9662-043f094a402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import argparse\n",
    "import gc\n",
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from colorama import init, Fore, Style\n",
    "\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from kiwipiepy import Kiwi\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2343fd14-217d-47e9-b7b6-5de38e623079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dad844a-11af-48b2-8a5e-ebd838124f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Chapter:\n",
    "    \"\"\"챕터 정보를 담는 데이터 클래스\"\"\"\n",
    "\n",
    "    idx: int\n",
    "    text: str\n",
    "    sents: list[str]\n",
    "    embeddings: list[float] | None = None\n",
    "\n",
    "    def __init__(self, idx, text, sents):\n",
    "        self.idx = idx\n",
    "        self.text = text\n",
    "        self.sents = sents\n",
    "        self.embedding = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ae5dc7-da0e-4d03-abe4-104904b25ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:992)>\n"
     ]
    }
   ],
   "source": [
    "# 초기화\n",
    "init(autoreset=True)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10e743cb-e7df-4ff5-b801-9cfb60a90583",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    input_dir: str\n",
    "    device: str\n",
    "    threshold: float\n",
    "    max_merge_n: int\n",
    "    merge_bonus: float\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        self.input_dir = args.input_dir\n",
    "        self.device = args.device\n",
    "        self.threshold = args.threshold\n",
    "        self.max_merge_n = args.max_merge_n\n",
    "        self.merge_bonus = args.merge_bonus\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b504f6a3-3779-4b30-be69-9612b5d6352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalAligner:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = args.device\n",
    "        \n",
    "        print(f\"{Fore.CYAN}[System] Loading Models...{Style.RESET_ALL}\")\n",
    "        self.kiwi = Kiwi()\n",
    "        self.model = SentenceTransformer('sentence-transformers/LaBSE', device=self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.total_matches = 0\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if not text: return \"\"\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        for tag in soup([\"script\", \"style\", \"nav\", \"sup\", \"footer\", \"header\", \"table\"]):\n",
    "            tag.decompose()\n",
    "        text = soup.get_text()\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    def load_epub_chapters(self, filepath: str, lang: str) -> list[Chapter]:\n",
    "        try:\n",
    "            book = epub.read_epub(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"{Fore.RED}[Error] Failed to read {filepath}: {e}{Style.RESET_ALL}\")\n",
    "            return []\n",
    "\n",
    "        chapters = []\n",
    "        chap_idx = 0\n",
    "\n",
    "        for item_id, _ in book.spine:\n",
    "            item = book.get_item_with_id(item_id)\n",
    "            if not item: continue\n",
    "            \n",
    "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "                content = item.get_content().decode('utf-8', errors='ignore')\n",
    "                cleaned = self.clean_text(content)\n",
    "                \n",
    "                if len(cleaned) < 50: continue\n",
    "\n",
    "                if lang == 'en':\n",
    "                    sents = sent_tokenize(cleaned)\n",
    "                else:\n",
    "                    sents = [s.text for s in self.kiwi.split_into_sents(cleaned)]\n",
    "                \n",
    "                sents = [s for s in sents if len(s.strip()) > 1]\n",
    "                \n",
    "                if len(sents) > 0:\n",
    "                    chapters.append(Chapter(chap_idx, cleaned, sents))\n",
    "                    chap_idx += 1\n",
    "        \n",
    "        return chapters\n",
    "\n",
    "    def get_chapter_embedding(self, chapter: Chapter):\n",
    "        summary = \" \".join(chapter.sents[:10]) \n",
    "        return self.model.encode(summary, convert_to_tensor=True, device=self.device)\n",
    "\n",
    "    def align_chapters(self, en_chapters, kr_chapters):\n",
    "        n_en = len(en_chapters)\n",
    "        n_kr = len(kr_chapters)\n",
    "        \n",
    "        if n_en == 0 or n_kr == 0: return []\n",
    "\n",
    "        print(f\"{Fore.YELLOW}[Chapter Align] Aligning {n_en} EN chapters with {n_kr} KR chapters...{Style.RESET_ALL}\")\n",
    "\n",
    "        en_embs = torch.stack([self.get_chapter_embedding(c) for c in en_chapters])\n",
    "        kr_embs = torch.stack([self.get_chapter_embedding(c) for c in kr_chapters])\n",
    "        \n",
    "        sim_matrix = util.cos_sim(en_embs, kr_embs).cpu().numpy()\n",
    "        \n",
    "        dp = np.full((n_en + 1, n_kr + 1), -np.inf)\n",
    "        backtrack = np.zeros((n_en + 1, n_kr + 1), dtype=int)\n",
    "        dp[0, 0] = 0\n",
    "        \n",
    "        SKIP_PENALTY = -0.2 \n",
    "\n",
    "        for i in range(n_en + 1):\n",
    "            for j in range(n_kr + 1):\n",
    "                if i == 0 and j == 0: continue\n",
    "                \n",
    "                best_score = -np.inf\n",
    "                best_action = 0 # 0:Stop, 1:Match, 2:Skip EN, 3:Skip KR\n",
    "\n",
    "                # Match\n",
    "                if i > 0 and j > 0:\n",
    "                    score = dp[i-1, j-1] + sim_matrix[i-1, j-1]\n",
    "                    if score > best_score:\n",
    "                        best_score, best_action = score, 1\n",
    "                \n",
    "                # Skip EN\n",
    "                if i > 0:\n",
    "                    score = dp[i-1, j] + SKIP_PENALTY\n",
    "                    if score > best_score:\n",
    "                        best_score, best_action = score, 2\n",
    "                \n",
    "                # Skip KR\n",
    "                if j > 0:\n",
    "                    score = dp[i, j-1] + SKIP_PENALTY\n",
    "                    if score > best_score:\n",
    "                        best_score, best_action = score, 3\n",
    "                \n",
    "                dp[i, j] = best_score\n",
    "                backtrack[i, j] = best_action\n",
    "        \n",
    "        matched_pairs = []\n",
    "        curr_i, curr_j = n_en, n_kr\n",
    "        \n",
    "        while curr_i > 0 or curr_j > 0:\n",
    "            action = backtrack[curr_i, curr_j]\n",
    "            if action == 0: break\n",
    "            \n",
    "            if action == 1: # Match\n",
    "                sim = sim_matrix[curr_i-1, curr_j-1]\n",
    "                if sim > 0.3: \n",
    "                    matched_pairs.append((en_chapters[curr_i-1], kr_chapters[curr_j-1]))\n",
    "                curr_i -= 1; curr_j -= 1\n",
    "            elif action == 2: # Skip EN\n",
    "                curr_i -= 1\n",
    "            elif action == 3: # Skip KR\n",
    "                curr_j -= 1\n",
    "            else:\n",
    "                curr_i -= 1; curr_j -= 1\n",
    "                \n",
    "        matched_pairs.reverse()\n",
    "        return matched_pairs\n",
    "\n",
    "    def align_sentences_in_chapter(self, en_sents, kr_sents, json_file, log_file):\n",
    "        if not en_sents or not kr_sents: return\n",
    "\n",
    "        en_emb = self.model.encode(en_sents, convert_to_tensor=True, device=self.device)\n",
    "        kr_emb = self.model.encode(kr_sents, convert_to_tensor=True, device=self.device)\n",
    "        \n",
    "        n_en = len(en_sents)\n",
    "        n_kr = len(kr_sents)\n",
    "        max_n = self.args.max_merge_n # 옵션에서 가져오기\n",
    "        \n",
    "        sim_matrix = util.cos_sim(en_emb, kr_emb).cpu().numpy()\n",
    "        dp = np.full((n_en + 1, n_kr + 1), -np.inf)\n",
    "        backtrack = np.zeros((n_en + 1, n_kr + 1), dtype=int)\n",
    "        dp[0, 0] = 0\n",
    "        \n",
    "        SKIP_PENALTY = -0.3\n",
    "        \n",
    "        # Action Codes:\n",
    "        # 1: 1:1 Match\n",
    "        # 2: Skip EN\n",
    "        # 3: Skip KR\n",
    "        # 10 + k: 1:k Match (Merge KR k sentences) -> ex) 12: 1:2, 13: 1:3\n",
    "        # 20 + k: k:1 Match (Merge EN k sentences) -> ex) 22: 2:1, 23: 3:1\n",
    "\n",
    "        for r in range(n_en + 1):\n",
    "            for c in range(n_kr + 1):\n",
    "                if r == 0 and c == 0: continue\n",
    "                best_score = -np.inf\n",
    "                best_action = 0 \n",
    "                \n",
    "                # (1) Skip EN\n",
    "                if r > 0:\n",
    "                    if dp[r-1, c] + SKIP_PENALTY > best_score:\n",
    "                        best_score, best_action = dp[r-1, c] + SKIP_PENALTY, 2\n",
    "                \n",
    "                # (2) Skip KR\n",
    "                if c > 0:\n",
    "                    if dp[r, c-1] + SKIP_PENALTY > best_score:\n",
    "                        best_score, best_action = dp[r, c-1] + SKIP_PENALTY, 3\n",
    "                        \n",
    "                # (3) 1:1 Match\n",
    "                if r > 0 and c > 0:\n",
    "                    s = dp[r-1, c-1] + sim_matrix[r-1, c-1]\n",
    "                    if s > best_score: best_score, best_action = s, 1\n",
    "                \n",
    "                # (4) 1:N Match (EN 1 : KR k)\n",
    "                # k: 2 ~ max_n\n",
    "                for k in range(2, max_n + 1):\n",
    "                    if r > 0 and c >= k:\n",
    "                        # KR 벡터 평균 계산\n",
    "                        vec_avg = torch.mean(kr_emb[c-k : c], dim=0)\n",
    "                        sim = util.cos_sim(en_emb[r-1], vec_avg).item()\n",
    "                        \n",
    "                        # 병합 보너스: (k-1) * merge_bonus\n",
    "                        # 예: 2개 합치면 1배, 3개 합치면 2배 보너스\n",
    "                        bonus = self.args.merge_bonus * (k - 1)\n",
    "                        s = dp[r-1, c-k] + sim + bonus\n",
    "                        \n",
    "                        if s > best_score:\n",
    "                            best_score, best_action = s, 10 + k\n",
    "\n",
    "                # (5) N:1 Match (EN k : KR 1)\n",
    "                for k in range(2, max_n + 1):\n",
    "                    if r >= k and c > 0:\n",
    "                        # EN 벡터 평균 계산\n",
    "                        vec_avg = torch.mean(en_emb[r-k : r], dim=0)\n",
    "                        sim = util.cos_sim(vec_avg, kr_emb[c-1]).item()\n",
    "                        \n",
    "                        bonus = self.args.merge_bonus * (k - 1)\n",
    "                        s = dp[r-k, c-1] + sim + bonus\n",
    "                        \n",
    "                        if s > best_score:\n",
    "                            best_score, best_action = s, 20 + k\n",
    "\n",
    "                dp[r, c] = best_score\n",
    "                backtrack[r, c] = best_action\n",
    "\n",
    "        # --- Backtracking ---\n",
    "        path = []\n",
    "        curr_r, curr_c = n_en, n_kr\n",
    "        \n",
    "        while curr_r > 0 or curr_c > 0:\n",
    "            action = backtrack[curr_r, curr_c]\n",
    "            if action == 0: break\n",
    "            \n",
    "            if action == 1: # 1:1\n",
    "                sim = dp[curr_r, curr_c] - dp[curr_r-1, curr_c-1]\n",
    "                path.append({'type': '1:1', 'en_idx': [curr_r-1], 'kr_idx': [curr_c-1], 'sim': sim})\n",
    "                curr_r -= 1; curr_c -= 1\n",
    "\n",
    "            elif action == 2: # Skip EN\n",
    "                path.append({'type': 'SKIP_EN', 'en_idx': [curr_r-1], 'kr_idx': [], 'sim': 0})\n",
    "                curr_r -= 1\n",
    "\n",
    "            elif action == 3: # Skip KR\n",
    "                curr_c -= 1\n",
    "            \n",
    "            elif action >= 20: # k:1 Match (EN k개 병합)\n",
    "                k = action - 20\n",
    "                sim_total = dp[curr_r, curr_c] - dp[curr_r-k, curr_c-1]\n",
    "                # 보너스 제외한 순수 유사도 추정 (보너스 빼기)\n",
    "                bonus = self.args.merge_bonus * (k - 1)\n",
    "                real_sim = sim_total - bonus\n",
    "                \n",
    "                en_indices = list(range(curr_r-k, curr_r))\n",
    "                path.append({'type': f'{k}:1', 'en_idx': en_indices, 'kr_idx': [curr_c-1], 'sim': real_sim})\n",
    "                curr_r -= k; curr_c -= 1\n",
    "                \n",
    "            elif action >= 10: # 1:k Match (KR k개 병합)\n",
    "                k = action - 10\n",
    "                sim_total = dp[curr_r, curr_c] - dp[curr_r-1, curr_c-k]\n",
    "                bonus = self.args.merge_bonus * (k - 1)\n",
    "                real_sim = sim_total - bonus\n",
    "                \n",
    "                kr_indices = list(range(curr_c-k, curr_c))\n",
    "                path.append({'type': f'1:{k}', 'en_idx': [curr_r-1], 'kr_idx': kr_indices, 'sim': real_sim})\n",
    "                curr_r -= 1; curr_c -= k\n",
    "\n",
    "            else:\n",
    "                # Should not happen\n",
    "                curr_r -= 1; curr_c -= 1\n",
    "        \n",
    "        path.reverse()\n",
    "\n",
    "        # --- 파일 쓰기 ---\n",
    "        for item in path:\n",
    "            en_text_list = [en_sents[i] for i in item['en_idx']]\n",
    "            en_text_full = \" \".join(en_text_list)\n",
    "            \n",
    "            if item['type'] == 'SKIP_EN':\n",
    "                log_msg = f\"[SKIP] (Score: {item['sim']:.4f})\\n EN: {en_text_full}\\n KR: (No Match)\\n\"\n",
    "                log_file.write(log_msg + \"-\"*50 + \"\\n\")\n",
    "            else:\n",
    "                kr_text_list = [kr_sents[i] for i in item['kr_idx']]\n",
    "                kr_text_full = \" \".join(kr_text_list)\n",
    "                \n",
    "                sim_score = item['sim']\n",
    "                status = \"MATCH\" if sim_score >= self.args.threshold else \"REJECT\"\n",
    "                \n",
    "                log_msg = f\"[{status}] (Score: {sim_score:.4f} | Type: {item['type']})\\n EN: {en_text_full}\\n KR: {kr_text_full}\\n\"\n",
    "                log_file.write(log_msg + \"-\"*50 + \"\\n\")\n",
    "\n",
    "                if sim_score >= self.args.threshold:\n",
    "                    data = {\n",
    "                        \"system\": \"전문 번역가 스타일로 번역하세요.\",\n",
    "                        \"user\": en_text_full,\n",
    "                        \"assistant\": kr_text_full\n",
    "                    }\n",
    "                    json_file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                    self.total_matches += 1\n",
    "\n",
    "    def process_pair(self, en_path, kr_path):\n",
    "        book_title = os.path.basename(en_path).replace('en_', '').replace('.epub', '')\n",
    "        output_json = f\"dataset_{book_title}.jsonl\"\n",
    "        output_log = f\"dataset_{book_title}.log\"\n",
    "        \n",
    "        print(f\"\\n{Fore.BLUE}=== Processing: {book_title} ==={Style.RESET_ALL}\")\n",
    "        \n",
    "        en_chapters = self.load_epub_chapters(en_path, 'en')\n",
    "        kr_chapters = self.load_epub_chapters(kr_path, 'kr')\n",
    "        \n",
    "        print(f\" - Loaded Chapters: EN={len(en_chapters)}, KR={len(kr_chapters)}\")\n",
    "\n",
    "        matched_chapters = self.align_chapters(en_chapters, kr_chapters)\n",
    "        print(f\" - Matched Chapters: {len(matched_chapters)} pairs\")\n",
    "\n",
    "        self.total_matches = 0\n",
    "        with open(output_json, 'w', encoding='utf-8') as f_json, \\\n",
    "             open(output_log, 'w', encoding='utf-8') as f_log:\n",
    "            \n",
    "            f_log.write(f\"=== Log for {book_title} (Max Merge: {self.args.max_merge_n}) ===\\n\\n\")\n",
    "            \n",
    "            pbar = tqdm(matched_chapters, desc=\"Aligning Chapters\")\n",
    "            for en_chap, kr_chap in pbar:\n",
    "                f_log.write(f\"### Chapter Pair: EN[{en_chap.idx}] - KR[{kr_chap.idx}] ###\\n\")\n",
    "                self.align_sentences_in_chapter(en_chap.sents, kr_chap.sents, f_json, f_log)\n",
    "        \n",
    "        print(f\"{Fore.GREEN} >> Finished {book_title}. Total Matches: {self.total_matches}{Style.RESET_ALL}\")\n",
    "        print(f\" >> Log saved to: {output_log}\")\n",
    "\n",
    "    def run(self):\n",
    "        en_files = sorted(glob.glob(os.path.join(self.args.input_dir, \"en_*.epub\")))\n",
    "        for en_file in en_files:\n",
    "            kr_file = en_file.replace(\"en_\", \"kr_\")\n",
    "            if os.path.exists(kr_file):\n",
    "                self.process_pair(en_file, kr_file)\n",
    "            else:\n",
    "                print(f\"Skipping {en_file} (No pair found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92ebeb-69a8-4f7a-ad4a-5065d79f728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_dir', default='data')\n",
    "    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    parser.add_argument('--threshold', type=float, default=0.65, help='Similarity threshold')\n",
    "    \n",
    "    # [신규 옵션] N개 문장 병합 설정\n",
    "    parser.add_argument('--max_merge_n', type=int, default=5, help='Max sentences to merge (default: 5)')\n",
    "    parser.add_argument('--merge_bonus', type=float, default=0.001, help='Bonus for merging sentences')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if not os.path.exists(args.input_dir):\n",
    "        os.makedirs(args.input_dir)\n",
    "    \n",
    "    aligner = HierarchicalAligner(args)\n",
    "    aligner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c6c4af-a37c-4741-886d-96afdcaefcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[System] Loading Models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantization is not supported for ArchType::neon. Fall back to non-quantized model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing: Harry Potter and the Goblet of Fire ===\n",
      " - Loaded Chapters: EN=38, KR=43\n",
      "[Chapter Align] Aligning 38 EN chapters with 43 KR chapters...\n",
      " - Matched Chapters: 38 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning Chapters: 100%|████████████████████████████████████████████████████████████████████████████████| 38/38 [20:17<00:00, 32.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Finished Harry Potter and the Goblet of Fire. Total Matches: 5566\n",
      " >> Log saved to: dataset_Harry Potter and the Goblet of Fire.log\n",
      "\n",
      "=== Processing: Harry Potter and the Sorcerors Stone ===\n",
      " - Loaded Chapters: EN=18, KR=22\n",
      "[Chapter Align] Aligning 18 EN chapters with 22 KR chapters...\n",
      " - Matched Chapters: 17 pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aligning Chapters: 100%|████████████████████████████████████████████████████████████████████████████████| 17/17 [08:50<00:00, 31.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Finished Harry Potter and the Sorcerors Stone. Total Matches: 4485\n",
      " >> Log saved to: dataset_Harry Potter and the Sorcerors Stone.log\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "config.input_dir = \"data\"\n",
    "config.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "config.threshold = 0.65\n",
    "config.max_merge_n = 5\n",
    "config.merge_bonus = 0.001\n",
    "\n",
    "aligner = HierarchicalAligner(config)\n",
    "aligner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
